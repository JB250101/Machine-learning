{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 12 Lab - Vector Space Modelling, NLTK Tagsets and Tiknter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Space Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will walk through a simple example of Vector Space Modelling. Then we will use cosine similarity to find similarity between document and query and rank the documents accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = ['The quick brown fox jumps over the lazy dog.',\n",
    "               'A brown dog chased the fox.',\n",
    "               'The dog is lazy.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'],\n",
       " ['A', 'brown', 'dog', 'chased', 'the', 'fox', '.'],\n",
       " ['The', 'dog', 'is', 'lazy', '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## First step is to tokenize our text\n",
    "tokenized_documents = [word_tokenize(document) for document in sample_docs]\n",
    "tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.'],\n",
       " ['brown', 'dog', 'chased', 'fox', '.'],\n",
       " ['dog', 'lazy', '.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Second step is to calculate our TF-IDF \n",
    "## We need first to preprocess our text\n",
    "## For simplicity I will just remove the stop words in documents\n",
    "## and I will change words to lower\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "cleaned_data = [[word.lower() for word in document if word.lower() not in english_stopwords] for document in tokenized_documents]\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick brown fox jumps lazy dog .', 'brown dog chased fox .', 'dog lazy .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TF_IDF vectorizer takes as an input sentences, lets join our tokens\n",
    "cleaned_sentences = [' '.join(document) for document in cleaned_data]\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 12 stored elements and shape (3, 7)>\n",
      "  Coords\tValues\n",
      "  (0, 6)\t0.49482970636510465\n",
      "  (0, 0)\t0.37633074615060896\n",
      "  (0, 3)\t0.37633074615060896\n",
      "  (0, 4)\t0.49482970636510465\n",
      "  (0, 5)\t0.37633074615060896\n",
      "  (0, 2)\t0.29225439586501756\n",
      "  (1, 0)\t0.4804583972923858\n",
      "  (1, 3)\t0.4804583972923858\n",
      "  (1, 2)\t0.3731188059313277\n",
      "  (1, 1)\t0.6317450542765208\n",
      "  (2, 5)\t0.7898069290660905\n",
      "  (2, 2)\t0.6133555370249717 {'quick': 6, 'brown': 0, 'fox': 3, 'jumps': 4, 'lazy': 5, 'dog': 2, 'chased': 1}\n"
     ]
    }
   ],
   "source": [
    "## Lets define our vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_sentences)\n",
    "print(tfidf_matrix, tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'brown', 'dog']\n",
      "['brown', 'dog']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['brown dog']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Given that we have the TFIDF vectors, lets write the query and then get the vector of the query\n",
    "query = \"the brown dog\"\n",
    "## Preprocess the query\n",
    "query_tokens = word_tokenize(query)\n",
    "print(query_tokens)\n",
    "query_cleaned = [word.lower() for word in query_tokens if word.lower() not in english_stopwords]\n",
    "print(query_cleaned)\n",
    "query_cleaned_combined = [' '.join(query_cleaned)]\n",
    "query_cleaned_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 2 stored elements and shape (1, 7)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t0.7898069290660905\n",
      "  (0, 2)\t0.6133555370249717\n"
     ]
    }
   ],
   "source": [
    "## Get the TFIDF vector of the query\n",
    "query_tfIdf_vector = tfidf_vectorizer.transform(query_cleaned_combined)\n",
    "print(query_tfIdf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47648448, 0.60832386, 0.37620501]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we need to find the cosine similarity between the query and documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(query_tfIdf_vector, tfidf_matrix)\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The quick brown fox jumps over the lazy dog.',\n",
       "  np.float64(0.4764844828540594)),\n",
       " ('A brown dog chased the fox.', np.float64(0.6083238568956406)),\n",
       " ('The dog is lazy.', np.float64(0.37620501479919144))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To rank the documents, first we will create a list of ranked results\n",
    "results = [(sample_docs[i], cosine_similarities[0][i]) for i in range(len(sample_docs))]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A brown dog chased the fox.', np.float64(0.6083238568956406)),\n",
       " ('The quick brown fox jumps over the lazy dog.',\n",
       "  np.float64(0.4764844828540594)),\n",
       " ('The dog is lazy.', np.float64(0.37620501479919144))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sorting the results based on similarity to rank the documents\n",
    "results.sort(key=lambda x:x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hands on Exercise inClass:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the list of the following relevant and retrieved documents. Find the precision and recall of this retrieval system.<br>\n",
    "Assume that we only have the documents that we can see in relevant or retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of this system is: 0, recall of the system is 0\n"
     ]
    }
   ],
   "source": [
    "# Sample relevant documents and retrieved documents\n",
    "relevant_documents = [0, 1, 2, 4]\n",
    "retrieved_documents = [0, 3, 1,5, 7]\n",
    "##TODO: FIND THE FOLLOWING\n",
    "TP = 0\n",
    "precision =0\n",
    "recall = 0\n",
    "print(f'Precision of this system is: {precision}, recall of the system is {recall}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] []\n"
     ]
    }
   ],
   "source": [
    "## TODO: Calculate precision at k for the following k_values\n",
    "k_values = [1,3,5]\n",
    "precision_k_lists = []\n",
    "recall_k_lists = []\n",
    "\n",
    "print(precision_k_lists, recall_k_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average precision\n",
    "\n",
    "# First, find the k values where you need to calculate the precision at\n",
    "\n",
    "\n",
    "# Calculate the average precision (average of precision at k's)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate Mean_Average_Precision Given that another IR System returns the following results\n",
    "retrieved_documents_ir2 = [0, 3, 7, 2]\n",
    "# Calculate the average precision\n",
    "\n",
    "\n",
    "# First, find the k values where you need to calculate the precision at\n",
    "\n",
    "\n",
    "# Then, Calculate MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the list of the following documents and query. Find the cosine_similarity between documents and the query. Rank the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Natural language processing is a field of computer science.\",\n",
    "    \"Machine learning algorithms analyze data to make predictions.\",\n",
    "    \"Data preprocessing is essential for machine learning models.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"Information retrieval involves finding relevant information in a collection.\",\n",
    "    \"Neural networks are used in deep learning models.\",\n",
    "    \"Statistical analysis helps in understanding data patterns.\",\n",
    "    \"Big data technologies handle large volumes of data.\",\n",
    "    \"Classification and regression are types of supervised learning.\",\n",
    "    \"Clustering algorithms group similar data points together.\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kappa Measure Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator 1's relevance assessments\n",
    "annotator1 = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 indicates relevant, 0 indicates not relevant\n",
    "\n",
    "# Annotator 2's relevance assessments (with some disagreements)\n",
    "annotator2 = [1, 1, 0, 0, 1, 0, 1, 0, 1, 1]  # 1 indicates relevant, 0 indicates not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Kappa Score is: 0\n"
     ]
    }
   ],
   "source": [
    "## Calculate the Kappa Score of Confidence of the two annotations\n",
    "from sklearn.metrics import confusion_matrix\n",
    "kappa_score = 0\n",
    "print(f\"Your Kappa Score is: {kappa_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa for IR: 0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Compute Cohen's Kappa for relevance assessments\n",
    "kappa_ir = cohen_kappa_score(annotator1, annotator2)\n",
    "\n",
    "print(f\"Cohen's Kappa for IR: {kappa_ir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK POS Tagsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine\n",
      "[nltk_data]     learning\\.venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine\n",
      "[nltk_data]     learning\\.venv\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine\n",
      "[nltk_data]     learning\\.venv\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine\n",
      "[nltk_data]     learning\\.venv\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine\n",
      "[nltk_data]     learning\\.venv\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine\n",
      "[nltk_data]     learning\\.venv\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and POS Tagging:\n",
    "Demonstrate how to perform tokenization and POS tagging using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('in', 'IN'), ('Python', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "\n",
    "sentence = \"NLTK is a powerful library for natural language processing in Python.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagsets are systems that assign specific tags to words in a text based on their grammatical roles and parts of speech. They are essential in natural language processing (NLP) tasks as they provide valuable information about the structure and meaning of a sentence.<br>\n",
    "There are several POS tagsets available, each with its own set of tags and conventions. Let's explore four common POS tagsets:<br>\n",
    "\n",
    "1. Penn Treebank Tagset:<br>\n",
    "The Penn Treebank Tagset is one of the most widely used tagsets in NLP. It contains a large number of fine-grained tags, making it suitable for detailed analysis. It includes tags such as NN (noun), VB (verb), JJ (adjective), and many more.<br>\n",
    "\n",
    "2. Universal POS Tagset:<br>\n",
    "The Universal POS Tagset, as the name suggests, aims to be more universal and less language-dependent. It reduces the number of tags compared to the Penn Treebank Tagset, making it simpler and easier to use across different languages. It includes tags such as NOUN, VERB, ADJ, and others.<br>\n",
    "\n",
    "3. Brown Corpus Tagset:<br>\n",
    "The Brown Corpus Tagset is based on the Brown Corpus, a corpus of English text with tags assigned to each word. It includes tags like NN (noun), VB (verb), JJ (adjective), and others, similar to the Penn Treebank Tagset.<br>\n",
    "\n",
    "4. WordNet POS Tagset:<br>\n",
    "WordNet is a lexical database that categorizes words into sets of synonyms called synsets. The WordNet POS Tagset is used to represent the POS of a word in WordNet. It includes tags like NOUN, VERB, ADJ, ADV, and others.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use some of the taggers with NLTK library in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fox', 'Animal'), ('The', 'Article'), ('Day', 'Word'), ('Broke', None)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    [('Fox', 'Animal'), ('Day', 'Word'), ('The', 'Article'), ('The', 'Article')],\n",
    "    [('Fox', 'Animal'), ('Day', 'Article'), ('The', 'Word'), ('The', 'Article')],\n",
    "    [('Fox', 'Word'), ('Day', 'Word'), ('The', 'Word'), ('The', 'Word')]\n",
    "]\n",
    "tagger_uni = UnigramTagger(corpus)\n",
    "tagger_uni.tag(['Fox', 'The','Day', 'Broke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "Fulton\n",
      "County\n",
      "Grand\n",
      "Jury\n",
      "said\n",
      "Friday\n",
      "an\n",
      "investigation\n",
      "of\n",
      "Atlanta's\n",
      "recent\n",
      "primary\n",
      "election\n",
      "produced\n",
      "``\n",
      "no\n",
      "evidence\n",
      "''\n",
      "that\n",
      "any\n",
      "irregularities\n",
      "took\n",
      "place\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for (x,y) in nltk.corpus.brown.tagged_sents(categories='news')[0]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Tagset:\n",
      "[('The', 'AT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', None), ('over', 'IN'), ('the', 'AT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
      "Penn Treebank Tagset:\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n",
      "Universal POS Tagset:\n",
      "[('The', 'DET'), ('quick', 'ADJ'), ('brown', 'NOUN'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Using Unigram Tagger\n",
    "tag_res = []\n",
    "tagger_uni = UnigramTagger(brown.tagged_sents(categories='fiction'))\n",
    "for word, tag in tagger_uni.tag(word_tokenize(sentence)):\n",
    "    tag_res.append((word, tag)) \n",
    "print(\"Unigram Tagset:\")\n",
    "print(tag_res)\n",
    "\n",
    "# Using Penn Treebank Tagset\n",
    "tags_ptb = pos_tag(word_tokenize(sentence))\n",
    "print(\"Penn Treebank Tagset:\")\n",
    "print(tags_ptb)\n",
    "\n",
    "# Using Universal POS Tagset\n",
    "tags_universal = pos_tag(word_tokenize(sentence), tagset='universal')\n",
    "print(\"Universal POS Tagset:\")\n",
    "print(tags_universal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tkinter (Check at home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tkinter is a built-in Python library used for creating Graphical User Interfaces (GUI). It allows you to create windows, buttons, labels, textboxes, and other GUI elements to build interactive applications. Tkinter is easy to learn and is widely used for creating desktop applications and simple games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install tk if you don't have it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = tk.Tk() # initializes a tkinter window\n",
    "# window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = tk.Tk()\n",
    "label = tk.Label(window, text=\"Hello, Tkinter!\")\n",
    "button = tk.Button(window, text=\"Click Me!\")\n",
    "# window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometry Manager: Tkinter uses geometry managers to organize widgets within a window. The pack() method is the simplest geometry manager, which automatically arranges widgets in a horizontal or vertical stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.pack()\n",
    "button.pack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding event handlers: Tkinter allows you to define event handlers to respond to user actions like button clicks. For example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def button_click():\n",
    "    print(\"Button clicked!\")\n",
    "window = tk.Tk()\n",
    "label = tk.Label(window, text=\"Hello, Tkinter!\")\n",
    "\n",
    "button = tk.Button(window, text=\"Click Me!\", command=button_click)\n",
    "label.pack()\n",
    "button.pack()\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometry Management with Grid:\n",
    "An alternative to pack() is the grid() method, which allows you to create a more complex layout using rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window = tk.Tk()\n",
    "label = tk.Label(window, text=\"Hello, Tkinter!\")\n",
    "\n",
    "button = tk.Button(window, text=\"Click Me!\", command=button_click)\n",
    "label.grid(row=0, column=0)\n",
    "button.grid(row=0, column=1)\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing Application: To close the Tkinter window, simply click the close button (X) on the window or call the destroy() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a button for closing the window (DONE)\n",
    "\n",
    "def button_click_2():\n",
    "    window.destroy()\n",
    "window = tk.Tk()\n",
    "label = tk.Label(window, text=\"Hello, Tkinter!\")\n",
    "# Figure out what does height and width represent\n",
    "textBox = tk.Text(window, height = 10, width = 52)\n",
    "\n",
    "def extract_text():\n",
    "    # Figure out what does 1.0 represent\n",
    "    print(textBox.get(2.5, 'end-1c'))\n",
    "button = tk.Button(window, text=\"Click Me!\", command=extract_text)\n",
    "close_button = tk.Button(window, text=\"Close Window\", command=button_click_2)\n",
    "label.grid(row=0, column=0)\n",
    "button.grid(row=0, column=1)\n",
    "textBox.grid(row=1, column=0)\n",
    "close_button.grid(row=1, column=1)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more about tkinter: https://docs.python.org/3/library/tk.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO At HOME: Explore Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore fastText model from Gensim library - you can learn more on fasttext here(https://fasttext.cc/docs/en/python-module.html).\n",
    "Test FastText library for getting word embeddings of the same corpus of Word2Vec discussed in class. FastText is simply Word2Vec with better ability to capture out of words dictionary. \n",
    "- Import and test fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. \n",
    "\"\"\" \n",
    "text2 = \"\"\"Natural language processing (NLP) techniques aim to enable computers to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET WORD VECTOR FOR THE WORD NATURAL using GENSIM FastText"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
