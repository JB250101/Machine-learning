{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 13 Word Embeddings Tutorial\n",
    "Tutorial for extracting word embeddings from words.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec embeddings using the Gensim library.<br> \n",
    "    Word2Vec is a popular technique for learning word embeddings, which are dense vector representations of words that capture semantic relationships between words based on their context.<br>\n",
    "    As discussed, Word2Vec have 2 types, Skipgrams and CBOW. Where SkipGrams are trained to predict context words given the target word, however CBOW is trained to predict target words given its context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Requirements: downloading punkt from nltk, and installing gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The quick brown fox jumps over the lazy dog.\n",
    "## Target Word: Jumps\n",
    "## Context Words: Words in a context window size n --> n=2 [brown, fox, over, the]\n",
    "## SkipGram: \n",
    "#   jumps (input) --> 4 outputs [brown, fox, over, the]\n",
    "## CBOW: \n",
    "# [brown, fox, over, the] (inputs) --> 1 output (jumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing needed libraries\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', '(', 'ai', ')', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'nlp', 'techniques', 'aim', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'meaningful', 'and', 'contextually', 'relevant', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample Sentence\n",
    "text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. NLP techniques aim to enable computers to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.\n",
    "\"\"\"\n",
    "tokenized_words = word_tokenize(text.lower())\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model_word2Vec = Word2Vec(sentences=[tokenized_words], vector_size=50, window=5,  min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for language is [-1.04924932e-03  4.22840036e-04  1.01921493e-02  1.80208944e-02\n",
      " -1.85975824e-02 -1.42208599e-02  1.29707102e-02  1.79747008e-02\n",
      " -1.00614503e-02 -7.51019455e-03  1.47279548e-02 -3.08123045e-03\n",
      " -9.06075630e-03  1.31399417e-02 -9.73974820e-03 -3.60585051e-03\n",
      "  5.78764221e-03  2.02601845e-03 -1.65603217e-02 -1.89395323e-02\n",
      "  1.46390153e-02  1.01228226e-02  1.35796499e-02  1.54736219e-03\n",
      "  1.27078006e-02 -6.81248819e-03 -1.89916836e-03  1.15286848e-02\n",
      " -1.50743807e-02 -7.89453555e-03 -1.50234010e-02 -1.84506015e-03\n",
      "  1.90822110e-02 -1.46591011e-02 -4.66508837e-03 -3.87337268e-03\n",
      "  1.61433201e-02 -1.19048795e-02  7.79837355e-05 -9.55065712e-03\n",
      " -1.91739332e-02  9.98215098e-03 -1.75059494e-02 -8.77499487e-03\n",
      " -2.21634273e-05 -5.67505893e-04 -1.53313102e-02  1.92039497e-02\n",
      "  9.98303201e-03  1.84752923e-02]\n"
     ]
    }
   ],
   "source": [
    "# Get word vector for a specific word\n",
    "## Example: language\n",
    "word = 'language'\n",
    "wv_vector = model_word2Vec.wv[word]\n",
    "print(f\"Word Vector for {word} is {wv_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words are [('way', 0.27101483941078186), ('between', 0.2417445182800293), ('ai', 0.21129381656646729), ('of', 0.18660590052604675), ('natural', 0.1674286276102066), ('subfield', 0.16039668023586273), ('enable', 0.15051932632923126), ('on', 0.1452927589416504), ('is', 0.13285453617572784), ('computers', 0.12684977054595947)]\n"
     ]
    }
   ],
   "source": [
    "# Find similar words\n",
    "similar_words = model_word2Vec.wv.most_similar(word)\n",
    "print(f'Most similar words are {similar_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIPGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector skipgram is [-9.03553970e-04  6.09972230e-05  1.00390101e-02  1.80581324e-02\n",
      " -1.85802374e-02 -1.42736342e-02  1.31737618e-02  1.81576647e-02\n",
      " -1.02728289e-02 -7.52615416e-03  1.45699615e-02 -3.23969754e-03\n",
      " -8.90274160e-03  1.33267427e-02 -9.75288637e-03 -3.34877032e-03\n",
      "  5.90627361e-03  2.19281320e-03 -1.65371932e-02 -1.91209484e-02\n",
      "  1.47795556e-02  1.00894021e-02  1.37643609e-02  1.57578371e-03\n",
      "  1.28754247e-02 -6.85847225e-03 -1.91866246e-03  1.16009628e-02\n",
      " -1.51612358e-02 -8.00818577e-03 -1.49769830e-02 -1.86830643e-03\n",
      "  1.90617591e-02 -1.46751460e-02 -4.67565376e-03 -3.88333946e-03\n",
      "  1.62615869e-02 -1.20394714e-02  4.63891411e-05 -9.83959157e-03\n",
      " -1.90989133e-02  9.84797068e-03 -1.74361710e-02 -8.68283305e-03\n",
      "  1.56206806e-04 -4.42421937e-04 -1.53591009e-02  1.90436654e-02\n",
      "  1.01097953e-02  1.86039284e-02]\n",
      "Most similar words are [('way', 0.27579179406166077), ('between', 0.24940559267997742), ('ai', 0.21399669349193573), ('of', 0.18884523212909698), ('natural', 0.16923265159130096), ('subfield', 0.15933887660503387), ('enable', 0.15793290734291077), ('on', 0.14782708883285522), ('is', 0.13808724284172058), ('computers', 0.128434956073761)]\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "model_word2Vec_sg = Word2Vec(sentences=[tokenized_words], vector_size=50, window=5,  min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Get word vector for a specific word\n",
    "wv_skipgram = model_word2Vec_sg.wv[word] \n",
    "print(f'Word Vector skipgram is {wv_skipgram}')\n",
    "# Find similar words\n",
    "similar_words_sg = model_word2Vec_sg.wv.most_similar(word)\n",
    "print(f'Most similar words are {similar_words_sg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.03553970e-04,  6.09972230e-05,  1.00390101e-02,  1.80581324e-02,\n",
       "       -1.85802374e-02, -1.42736342e-02,  1.31737618e-02,  1.81576647e-02,\n",
       "       -1.02728289e-02, -7.52615416e-03,  1.45699615e-02, -3.23969754e-03,\n",
       "       -8.90274160e-03,  1.33267427e-02, -9.75288637e-03, -3.34877032e-03,\n",
       "        5.90627361e-03,  2.19281320e-03, -1.65371932e-02, -1.91209484e-02,\n",
       "        1.47795556e-02,  1.00894021e-02,  1.37643609e-02,  1.57578371e-03,\n",
       "        1.28754247e-02, -6.85847225e-03, -1.91866246e-03,  1.16009628e-02,\n",
       "       -1.51612358e-02, -8.00818577e-03, -1.49769830e-02, -1.86830643e-03,\n",
       "        1.90617591e-02, -1.46751460e-02, -4.67565376e-03, -3.88333946e-03,\n",
       "        1.62615869e-02, -1.20394714e-02,  4.63891411e-05, -9.83959157e-03,\n",
       "       -1.90989133e-02,  9.84797068e-03, -1.74361710e-02, -8.68283305e-03,\n",
       "        1.56206806e-04, -4.42421937e-04, -1.53591009e-02,  1.90436654e-02,\n",
       "        1.01097953e-02,  1.86039284e-02], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How to save/load skipgram and CBOW models?\n",
    "model_word2Vec_sg.save('skipgram_word2vec.bin')\n",
    "## Loading\n",
    "loaded_sg = Word2Vec.load('skipgram_word2vec.bin')\n",
    "loaded_sg.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use word2Vec for SkipGrams and CBOW? Explore whether they will give different results for similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Words based on Cooccurence Pattern --> Brown Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "# download brown corpus if not downloaded before\n",
    "# nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve sentences from brown corpus\n",
    "corpus = brown.sents()[:1]\n",
    "# transform all sentences to lower case\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]\n",
    "\n",
    "# Create a set of unique words in the corpus --> Vocab\n",
    "vocab = set(word for sent in corpus for word in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'fulton',\n",
       "  'county',\n",
       "  'grand',\n",
       "  'jury',\n",
       "  'said',\n",
       "  'friday',\n",
       "  'an',\n",
       "  'investigation',\n",
       "  'of',\n",
       "  \"atlanta's\",\n",
       "  'recent',\n",
       "  'primary',\n",
       "  'election',\n",
       "  'produced',\n",
       "  '``',\n",
       "  'no',\n",
       "  'evidence',\n",
       "  \"''\",\n",
       "  'that',\n",
       "  'any',\n",
       "  'irregularities',\n",
       "  'took',\n",
       "  'place',\n",
       "  '.']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({\"''\",\n",
       "  '.',\n",
       "  '``',\n",
       "  'an',\n",
       "  'any',\n",
       "  \"atlanta's\",\n",
       "  'county',\n",
       "  'election',\n",
       "  'evidence',\n",
       "  'friday',\n",
       "  'fulton',\n",
       "  'grand',\n",
       "  'investigation',\n",
       "  'irregularities',\n",
       "  'jury',\n",
       "  'no',\n",
       "  'of',\n",
       "  'place',\n",
       "  'primary',\n",
       "  'produced',\n",
       "  'recent',\n",
       "  'said',\n",
       "  'that',\n",
       "  'the',\n",
       "  'took'},\n",
       " 1.0,\n",
       " 1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to find co-occurence pattern, co-occurence matrix is needed to show the word count and which words does it co-occur with\n",
    "window = 2\n",
    "co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "vocab_list = list(vocab)\n",
    "for sentence in corpus:\n",
    "    for i,word in enumerate(sentence):\n",
    "        word_i_index = vocab_list.index(word)\n",
    "        for j in range(max(0, i-window), min(len(sentence),i+window+1)):\n",
    "            if i!=j:\n",
    "               word_j_index = vocab_list.index(sentence[j]) \n",
    "               co_occurrence_matrix[word_i_index, word_j_index]+=1\n",
    "vocab, co_occurrence_matrix[10,24], co_occurrence_matrix[24,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 16,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performing brown clustering, you need to set the number of clusters\n",
    "num_clusters = 3\n",
    "# assign each word as cluster\n",
    "cluster_assignment = np.arange(len(vocab))\n",
    "\n",
    "# Perform Brown clustering by recursively merging clusters\n",
    "for k in range(len(vocab)-num_clusters):\n",
    "    # Find the pair of clusters with the highest co-occurrence count\n",
    "    \n",
    "    i,j =np.unravel_index(co_occurrence_matrix.argmax(),co_occurrence_matrix.shape)\n",
    "    \n",
    "    # Merge the clusters by assigning the same cluster ID to both clusters\n",
    "    cluster_assignment[cluster_assignment==j]=i\n",
    "    \n",
    "    # Update the co-occurrence matrix by merging the counts of the two clusters\n",
    "\n",
    "    ## Summing up the row of i and j together into j\n",
    "    co_occurrence_matrix[i,:]+=co_occurrence_matrix[j,:]\n",
    "    ## Summing up the columns of i and j together into i\n",
    "    \n",
    "    co_occurrence_matrix[:,i]+=co_occurrence_matrix[:,j]\n",
    "    ## Set diagonal i,i =0\n",
    "    co_occurrence_matrix[i,i]=0\n",
    "    ## Set all elements in j=0\n",
    "    co_occurrence_matrix[j,:]=0\n",
    "    co_occurrence_matrix[:,j]=0\n",
    "\n",
    "# Final cluster assignments\n",
    "cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: DONT RUN FOR LARGE DATASET**<br>\n",
    "Visualizing brown clusters: <br>\n",
    "Install scipy if you didn't use it before. pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Islam\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:10: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code should be able to visualize brown clustering, however you need to experiment it with less words as it does many\n",
    "# computations for the linkage matrix.\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "# Create a linkage matrix for hierarchical clustering\n",
    "## Single: merge based on minimum distance between clusters\n",
    "## Complete: merge based on maximum distance\n",
    "## Average: Which averages the distance between elements in cluster i and j\n",
    "## Ward: Optimized method using Error Sum of Squares \n",
    "linkage_matrix = linkage(co_occurrence_matrix, method='average')\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(15,10))\n",
    "dendrogram(linkage_matrix, labels=list(vocab), orientation='right')\n",
    "plt.xlabel('Word Clusters')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Brown Clustering Dendogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe: \n",
    "Global Vectors for Word Representation is an unsupervised learning algorithm used to learn word embeddings from large amounts of text data. Word embeddings are dense vector representations of words that capture semantic relationships between words based on their co-occurrence statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Steps: Preprocess the text data.<br>\n",
    "Created the dictionary.<br>\n",
    "Traverse the glove file of a specific dimension and compare each word with all words in the dictionary,\n",
    "if a match occurs, copy the equivalent vector from the glove and paste into embedding_matrix at the corresponding index.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector for first word is =>  [ 0.44264999  0.84764999 -0.4598      0.67992997  0.13841     0.39456001\n",
      " -0.17343    -0.64055002  0.86439002  0.81624001  0.75738001  0.41143\n",
      "  1.09350002 -0.30068001 -0.08486     0.51784003  1.08700001  0.45061001\n",
      " -0.49595001 -0.60650003 -0.16749001 -0.28557    -0.043719   -0.86154002\n",
      "  0.3396     -0.75239998 -0.33206001  0.24668001  1.00209999  0.71923\n",
      "  3.30769992 -0.64754999  0.16509999 -0.91650999 -0.035363    0.21794\n",
      " -0.87897003  0.37801     0.66733003  0.42054    -0.21387     0.15917\n",
      "  0.52244997  0.20587    -0.16714001  0.58058    -0.36827999  0.035571\n",
      "  0.014099   -0.24817   ]\n"
     ]
    }
   ],
   "source": [
    "#Download Glove Pretrained Embeddings From: http://nlp.stanford.edu/data/glove.6B.zip  \n",
    "\n",
    "def embedding_for_vocab(filepath, word_index,\n",
    "                        embedding_dim):\n",
    "    vocab_size = len(word_index) + 1\n",
    "      \n",
    "    # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix_vocab = np.zeros((vocab_size,\n",
    "                                       embedding_dim))\n",
    "  \n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index.index(word)\n",
    "                embedding_matrix_vocab[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "  \n",
    "    return embedding_matrix_vocab\n",
    "  \n",
    "  \n",
    "# matrix for vocab: tokenized_words\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab(\n",
    "    './glove.6B/glove.6B.50d.txt', tokenized_words,\n",
    "  embedding_dim)\n",
    "  \n",
    "print(\"Dense vector for first word is => \",\n",
    "      embedding_matrix_vocab[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction --> SVD (LSA)\n",
    "    Latent Semantic Analysis (LSA) is a technique used in natural language processing to uncover the latent structure in a corpus of text documents by applying Singular Value Decomposition (SVD) to a term-document matrix. It allows us to reduce the dimensionality of the document-term space, thereby capturing the underlying semantic relationships between words and documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The dog barked at the fox.\",\n",
    "    'Artificial intelligence is a domain trying to mimic human activity.',\n",
    "    'NLP uses machine code to understand human language.',\n",
    "    \"The fox ran away quickly.\",\n",
    "    \"The dog is lazy.\",\n",
    "    \"NLP is a useful topic in Artificial Intelligence.\",\n",
    "    \"The fox is cunning.\",\n",
    "    'Machines are now utilized with Machine Learning techniques.'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Document-Term Matrix: We use the CountVectorizer from scikit-learn to convert the text documents into a document-term matrix. Each row in the matrix corresponds to a document, and each column represents a word's frequency in that document.\n",
    "\n",
    "- Apply LSA (SVD): We use the TruncatedSVD class from scikit-learn to perform Latent Semantic Analysis. We specify the number of components (dimensions) we want to reduce the feature space to (in this case, we use n_components=2 for simplicity).\n",
    "\n",
    "- Normalize Data: To ensure that each row in the transformed matrix has unit norm, we use the Normalizer from scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 30)\t2\n",
      "  (0, 26)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 30)\t2\n",
      "  (1, 11)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 33)\t1\n",
      "  (2, 31)\t1\n",
      "  (2, 22)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 0)\t1\n",
      "  (3, 31)\t1\n",
      "  (3, 12)\t1\n",
      "  (3, 23)\t1\n",
      "  :\t:\n",
      "  (4, 4)\t1\n",
      "  (4, 27)\t1\n",
      "  (5, 30)\t1\n",
      "  (5, 18)\t1\n",
      "  (5, 9)\t1\n",
      "  (5, 15)\t1\n",
      "  (6, 2)\t1\n",
      "  (6, 14)\t1\n",
      "  (6, 15)\t1\n",
      "  (6, 23)\t1\n",
      "  (6, 35)\t1\n",
      "  (6, 32)\t1\n",
      "  (6, 13)\t1\n",
      "  (7, 30)\t1\n",
      "  (7, 11)\t1\n",
      "  (7, 15)\t1\n",
      "  (7, 8)\t1\n",
      "  (8, 20)\t1\n",
      "  (8, 21)\t1\n",
      "  (8, 1)\t1\n",
      "  (8, 24)\t1\n",
      "  (8, 37)\t1\n",
      "  (8, 38)\t1\n",
      "  (8, 19)\t1\n",
      "  (8, 29)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.97437412, -0.40147243],\n",
       "       [ 2.41391749, -0.31049656],\n",
       "       [ 0.33098849,  2.53187627],\n",
       "       [ 0.07638947,  1.5820206 ],\n",
       "       [ 1.31085395, -0.1798731 ],\n",
       "       [ 1.44123562,  0.24648765],\n",
       "       [ 0.28621921,  1.75272387],\n",
       "       [ 1.35128013,  0.26753324],\n",
       "       [ 0.00611473,  0.34927119]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply SVD (Latent Semantic Analysis)\n",
    "n_components = 2  # Number of components after reducing dimensions\n",
    "lsa = TruncatedSVD(n_components)\n",
    "X_lsa = lsa.fit_transform(X)\n",
    "X_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSA Reduced Dimensionality:\n",
      "[[ 0.9910132  -0.1337641 ]\n",
      " [ 0.99182872 -0.12757661]\n",
      " [ 0.12962558  0.99156301]\n",
      " [ 0.04822982  0.99883627]\n",
      " [ 0.99071647 -0.13594439]\n",
      " [ 0.98568839  0.16857758]\n",
      " [ 0.16116492  0.98692749]\n",
      " [ 0.98095895  0.19421519]\n",
      " [ 0.01750442  0.99984679]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the transformed data\n",
    "lsa_pipeline = make_pipeline(lsa, Normalizer(copy=False))\n",
    "X_lsa_normalized = lsa_pipeline.fit_transform(X)\n",
    "print(\"\\nLSA Reduced Dimensionality:\")\n",
    "print(X_lsa_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Islam\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\lib\\cudnn_cnn_infer64_8.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)\n",
      "\u001b[1;32m<ipython-input-1-0fc28fb30e41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load pre-trained model tokenizer (vocabulary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;32mc:\\Users\\Islam\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m    122\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWinError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    123\u001b[0m                 \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf' Error loading \"{dll}\" or one of its dependencies.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m--> 124\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m    126\u001b[0m                 \u001b[0mis_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"c:\\Users\\Islam\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\lib\\cudnn_cnn_infer64_8.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT is a pretrained model that expects that the input data is in a specific format.\n",
    "- The data format is as follows:\n",
    "1. [SEP] is a special token that is used to mark the end of a sentence or the seperation between two sentences.\n",
    "2. [CLS] is a special token that marks the beginning of the text. \n",
    "3. Tokens must conform with the fixed vocab. used in BERT\n",
    "4. TokenIDs should be retrieved from BertTokenizer\n",
    "5. MaskIDs which mark tokens vs padding elements\n",
    "6. SegmentID represents different sentences.\n",
    "7. Position Embeddings used to show token position within the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The quick brown fox jumping over the lazy dog. The dog barked at the fox extract embeddings.\",\n",
    "    \"The fox ran away quickly.\",\n",
    "    \"The dog is lazy.\",\n",
    "    \"The fox is cunning.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] The quick brown fox jumping over the lazy dog [SEP]. The dog barked at the fox extract embeddings [SEP].'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_sentence = corpus[0]\n",
    "## Step 1: Adding the special token\n",
    "## Adding a CLS is simple\n",
    "sample_sentence = \"[CLS] \"+sample_sentence\n",
    "sample_sentence\n",
    "## To add the SEP:\n",
    "split_sentence = sample_sentence.split('.')\n",
    "sample_sentence=\" [SEP].\".join(split_sentence)\n",
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'the',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'jumping',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " '[SEP]',\n",
       " '.',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'barked',\n",
       " 'at',\n",
       " 'the',\n",
       " 'fox',\n",
       " 'extract',\n",
       " 'em',\n",
       " '##bed',\n",
       " '##ding',\n",
       " '##s',\n",
       " '[SEP]',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## To tokenize the text, BERT provides its own tokenizer\n",
    "## Which we already imported\n",
    "tokenized_sentence = tokenizer.tokenize(sample_sentence)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  'em',\n",
    "#  '##bed',\n",
    "#  '##ding',\n",
    "#  '##s',\n",
    "\n",
    "# embeddings not a common english word\n",
    "# WordPiece model divides it into common subwords\n",
    "# 'em'\n",
    "# '##bed'\n",
    "# '##ding'\n",
    "# '##s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: Some words has been split into smaller subwords and characters. The <b>two hash signs</b> preceding some of these subwords are just the tokenizer’s way to denote that this subword or character is part of a larger word and preceded by another subword. So, for example, the ‘##bed’ token is separate from the ‘bed’ token; the first is used whenever the subword ‘bed’ occurs within a larger word and the second is used explicitly for when the standalone token ‘thing you sleep on’ occurs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why?\n",
    "BERT is created using WordPiece model, where WordPiece model generated a vocabulary that contains all English characters plus the ~30,000 most common words and subwords found in the English language corpus the model is trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tears',\n",
       " 'senate',\n",
       " '00',\n",
       " 'card',\n",
       " 'asian',\n",
       " 'agent',\n",
       " '1947',\n",
       " 'software',\n",
       " '44',\n",
       " 'draw',\n",
       " 'warm',\n",
       " 'supposed',\n",
       " 'com',\n",
       " 'pro',\n",
       " '##il']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[4000:4015]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1996,\n",
       " 4248,\n",
       " 2829,\n",
       " 4419,\n",
       " 8660,\n",
       " 2058,\n",
       " 1996,\n",
       " 13971,\n",
       " 3899,\n",
       " 102,\n",
       " 1012,\n",
       " 1996,\n",
       " 3899,\n",
       " 17554,\n",
       " 2012,\n",
       " 1996,\n",
       " 4419,\n",
       " 14817,\n",
       " 7861,\n",
       " 8270,\n",
       " 4667,\n",
       " 2015,\n",
       " 102,\n",
       " 1012]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## To convert the tokens into Vocab Indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "indexed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]      :   101    \n",
      "the        :   1996   \n",
      "quick      :   4248   \n",
      "brown      :   2829   \n",
      "fox        :   4419   \n",
      "jumping    :   8660   \n",
      "over       :   2058   \n",
      "the        :   1996   \n",
      "lazy       :  13971   \n",
      "dog        :   3899   \n",
      "[SEP]      :   102    \n",
      ".          :   1012   \n",
      "the        :   1996   \n",
      "dog        :   3899   \n",
      "barked     :  17554   \n",
      "at         :   2012   \n",
      "the        :   1996   \n",
      "fox        :   4419   \n",
      "extract    :  14817   \n",
      "em         :   7861   \n",
      "##bed      :   8270   \n",
      "##ding     :   4667   \n",
      "##s        :   2015   \n",
      "[SEP]      :   102    \n",
      ".          :   1012   \n"
     ]
    }
   ],
   "source": [
    "for tup in zip(tokenized_sentence, indexed_tokens):\n",
    "    # Spacing can be added in formatted text using :<and size of spacing\n",
    "    print('{:<10} :{:^10}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "## Now we need to assign segment ID to tokens\n",
    "segments_ids = [1] * len(tokenized_sentence)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to convert the data we have into torch tensors, and then\n",
    "# Load the model\n",
    "torch_tensor = torch.tensor([indexed_tokens])\n",
    "segment_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "model=BertModel.from_pretrained('bert-base-uncased',\n",
    "                                output_hidden_states=True) \n",
    "## output_hidden_states tells the model to return all hidden states\n",
    "# Put the model in \"evaluation\" mode [feed-forward operation]\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produce from the 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(torch_tensor, segment_tensor)\n",
    "\n",
    "\n",
    "    # Because output_hidden_states is set to true, the third item will be hidden\n",
    "    # states from all layers, different configurations could be used when\n",
    "    # calling from_pretrained\n",
    "    hidden_states = outputs[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 25\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "# Understanding the output\n",
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[0]))\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[0][0]))\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusion: You can make use of a torch of size [25, 13, 768 ]\n",
    "So, for each token, ( 13,768) vector can represent it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [-0.4367,  0.5360, -0.0514,  ..., -0.0397,  0.6783, -0.5318],\n",
      "         [ 0.0764, -1.3345, -0.1239,  ..., -0.5126,  1.4597, -0.7255],\n",
      "         ...,\n",
      "         [-0.4762, -0.0660,  0.4777,  ...,  0.0764, -0.2680,  0.1501],\n",
      "         [-0.2840,  0.2271,  0.0111,  ..., -0.1397,  0.0289, -0.2636],\n",
      "         [-0.3635,  0.4525, -0.2429,  ...,  0.5196,  0.5170,  0.5195]]])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
