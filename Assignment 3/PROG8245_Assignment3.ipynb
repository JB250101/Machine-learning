{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Exploring IR & NLP\n",
    "- In this assignment we are going to implement various IR techniques <b><i>From Scratch</i></b>, Please don't use available libraries except if specified that you can use it.\n",
    "- You are required to submit 6 different functions for this assignment, you can additional helper functions but only 6 will be tested.\n",
    "- You will be granted 10 marks for clean code and documenting the code.\n",
    "- Student Name: Jaiv Burman \n",
    "- ID: 8930180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language, python proved its importance in various domains.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in python code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine\n",
      "[nltk_data]     learning\\.venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import ne_chunk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART A: Preprocessing (15 Marks)\n",
    "- You are required to preprocess the text and apply the tokenization process.<br/>\n",
    "- Proprocessing should include tokenization, normalization, stemming <b>OR</b> lemmatization, and Named Entity Recognition (NER).<br/>\n",
    "- You need to make sure that Named Entities are not broken into separate tokens, but should be normalized by case-folding only. <br/>\n",
    "- The output of this step should be list of tokenized sentences. [[sentence1_token1, sentence1_token2, .. .], [sentence2_token1, .. .], .. .] <br/>\n",
    "- Please write the functionality of clean_sentences as explained in the comment (Please do comment your code at each essential step) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['python',\n",
       "  'is',\n",
       "  'a',\n",
       "  'versatile',\n",
       "  'programming',\n",
       "  'language',\n",
       "  'python',\n",
       "  'proved',\n",
       "  'it',\n",
       "  'importance',\n",
       "  'in',\n",
       "  'various',\n",
       "  'domain'],\n",
       " ['javascript', 'is', 'widely', 'used', 'for', 'web', 'development'],\n",
       " ['java', 'is', 'known', 'for', 'it', 'platform', 'independence'],\n",
       " ['programming', 'involves', 'writing', 'code', 'to', 'solve', 'problem'],\n",
       " ['data', 'structure', 'are', 'crucial', 'for', 'efficient', 'programming'],\n",
       " ['algorithms',\n",
       "  'are',\n",
       "  'step-by-step',\n",
       "  'instruction',\n",
       "  'for',\n",
       "  'solving',\n",
       "  'problem'],\n",
       " ['version',\n",
       "  'control',\n",
       "  'system',\n",
       "  'help',\n",
       "  'manage',\n",
       "  'code',\n",
       "  'change',\n",
       "  'in',\n",
       "  'collaboration'],\n",
       " ['debugging',\n",
       "  'is',\n",
       "  'the',\n",
       "  'process',\n",
       "  'of',\n",
       "  'finding',\n",
       "  'and',\n",
       "  'fixing',\n",
       "  'error',\n",
       "  'in',\n",
       "  'python',\n",
       "  'code'],\n",
       " ['web',\n",
       "  'framework',\n",
       "  'simplify',\n",
       "  'the',\n",
       "  'development',\n",
       "  'of',\n",
       "  'web',\n",
       "  'application'],\n",
       " ['artificial',\n",
       "  'intelligence',\n",
       "  'can',\n",
       "  'be',\n",
       "  'applied',\n",
       "  'in',\n",
       "  'various',\n",
       "  'programming',\n",
       "  'task']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You are allowed for PART A to use any library that would help you in the task.\n",
    "def clean_sentences(sentence=None):\n",
    "    ## This function takes as an input list of sentences\n",
    "    ## This function returns a list of tokenized_sentences\n",
    "    # stemmer = PorterStemmer()\n",
    "    # test_string=\" \".join(sentence)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokenized_sentence=[]\n",
    "    for sentences in sentence:\n",
    "        word_tokens = word_tokenize(sentences) \n",
    "        punctuation_removed = [word for word in word_tokens if word not in string.punctuation]\n",
    "        lemmatize_words = [lemmatizer.lemmatize(word) for word in punctuation_removed]\n",
    "        case_folded = [word.lower() for word in lemmatize_words]\n",
    "        # stemmed_words = [stemmer.stem(word) for word in case_folded]\n",
    "        tokenized_sentence.append(case_folded)\n",
    "\n",
    "    return tokenized_sentence    \n",
    "\n",
    "clean_sentences(sentence=sample_sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = clean_sentences(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART B: Building IR Sentence-Word Representation (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>inverted index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': [1, 1, 8],\n",
       " 'is': [1, 2, 3, 8],\n",
       " 'a': [1],\n",
       " 'versatile': [1],\n",
       " 'programming': [1, 4, 5, 10],\n",
       " 'language': [1],\n",
       " 'proved': [1],\n",
       " 'it': [1, 3],\n",
       " 'importance': [1],\n",
       " 'in': [1, 7, 8, 10],\n",
       " 'various': [1, 10],\n",
       " 'domain': [1],\n",
       " 'javascript': [2],\n",
       " 'widely': [2],\n",
       " 'used': [2],\n",
       " 'for': [2, 3, 5, 6],\n",
       " 'web': [2, 9, 9],\n",
       " 'development': [2, 9],\n",
       " 'java': [3],\n",
       " 'known': [3],\n",
       " 'platform': [3],\n",
       " 'independence': [3],\n",
       " 'involves': [4],\n",
       " 'writing': [4],\n",
       " 'code': [4, 7, 8],\n",
       " 'to': [4],\n",
       " 'solve': [4],\n",
       " 'problem': [4, 6],\n",
       " 'data': [5],\n",
       " 'structure': [5],\n",
       " 'are': [5, 6],\n",
       " 'crucial': [5],\n",
       " 'efficient': [5],\n",
       " 'algorithms': [6],\n",
       " 'step-by-step': [6],\n",
       " 'instruction': [6],\n",
       " 'solving': [6],\n",
       " 'version': [7],\n",
       " 'control': [7],\n",
       " 'system': [7],\n",
       " 'help': [7],\n",
       " 'manage': [7],\n",
       " 'change': [7],\n",
       " 'collaboration': [7],\n",
       " 'debugging': [8],\n",
       " 'the': [8, 9],\n",
       " 'process': [8],\n",
       " 'of': [8, 9],\n",
       " 'finding': [8],\n",
       " 'and': [8],\n",
       " 'fixing': [8],\n",
       " 'error': [8],\n",
       " 'framework': [9],\n",
       " 'simplify': [9],\n",
       " 'application': [9],\n",
       " 'artificial': [10],\n",
       " 'intelligence': [10],\n",
       " 'can': [10],\n",
       " 'be': [10],\n",
       " 'applied': [10],\n",
       " 'task': [10]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    inverted_index = {}\n",
    "    \n",
    "    for id, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "                \n",
    "            inverted_index[token].append(id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "get_inverted_index(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentence = get_inverted_index(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>Positional index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': {1: [0, 6], 8: [10]},\n",
       " 'is': {1: [1], 2: [1], 3: [1], 8: [1]},\n",
       " 'a': {1: [2]},\n",
       " 'versatile': {1: [3]},\n",
       " 'programming': {1: [4], 4: [0], 5: [6], 10: [7]},\n",
       " 'language': {1: [5]},\n",
       " 'proved': {1: [7]},\n",
       " 'it': {1: [8], 3: [4]},\n",
       " 'importance': {1: [9]},\n",
       " 'in': {1: [10], 7: [7], 8: [9], 10: [5]},\n",
       " 'various': {1: [11], 10: [6]},\n",
       " 'domain': {1: [12]},\n",
       " 'javascript': {2: [0]},\n",
       " 'widely': {2: [2]},\n",
       " 'used': {2: [3]},\n",
       " 'for': {2: [4], 3: [3], 5: [4], 6: [4]},\n",
       " 'web': {2: [5], 9: [0, 6]},\n",
       " 'development': {2: [6], 9: [4]},\n",
       " 'java': {3: [0]},\n",
       " 'known': {3: [2]},\n",
       " 'platform': {3: [5]},\n",
       " 'independence': {3: [6]},\n",
       " 'involves': {4: [1]},\n",
       " 'writing': {4: [2]},\n",
       " 'code': {4: [3], 7: [5], 8: [11]},\n",
       " 'to': {4: [4]},\n",
       " 'solve': {4: [5]},\n",
       " 'problem': {4: [6], 6: [6]},\n",
       " 'data': {5: [0]},\n",
       " 'structure': {5: [1]},\n",
       " 'are': {5: [2], 6: [1]},\n",
       " 'crucial': {5: [3]},\n",
       " 'efficient': {5: [5]},\n",
       " 'algorithms': {6: [0]},\n",
       " 'step-by-step': {6: [2]},\n",
       " 'instruction': {6: [3]},\n",
       " 'solving': {6: [5]},\n",
       " 'version': {7: [0]},\n",
       " 'control': {7: [1]},\n",
       " 'system': {7: [2]},\n",
       " 'help': {7: [3]},\n",
       " 'manage': {7: [4]},\n",
       " 'change': {7: [6]},\n",
       " 'collaboration': {7: [8]},\n",
       " 'debugging': {8: [0]},\n",
       " 'the': {8: [2], 9: [3]},\n",
       " 'process': {8: [3]},\n",
       " 'of': {8: [4], 9: [5]},\n",
       " 'finding': {8: [5]},\n",
       " 'and': {8: [6]},\n",
       " 'fixing': {8: [7]},\n",
       " 'error': {8: [8]},\n",
       " 'framework': {9: [1]},\n",
       " 'simplify': {9: [2]},\n",
       " 'application': {9: [7]},\n",
       " 'artificial': {10: [0]},\n",
       " 'intelligence': {10: [1]},\n",
       " 'can': {10: [2]},\n",
       " 'be': {10: [3]},\n",
       " 'applied': {10: [4]},\n",
       " 'task': {10: [8]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    positional_index = {}\n",
    "    \n",
    "    for id, tokens in enumerate(list_of_sentence_tokens, start =1):\n",
    "        for position, token in enumerate(tokens):\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = {}\n",
    "                \n",
    "            if id not in positional_index[token]:\n",
    "                positional_index[token][id] = []\n",
    "                \n",
    "            positional_index[token][id].append(position)\n",
    "            \n",
    "    return positional_index\n",
    "    \n",
    "get_positional_index(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Question B-3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>TF-IDF Matrix</b> that is sufficient to represent the documents, the tokens are expected to be sorted as well as documentIDs. Assume that each sentence is a document and the sentence ID starts from 1. (10) You are not allowed to use any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.17712193023031123,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17712193023031123,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17712193023031123,\n",
       "  0.0704839024518581,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0704839024518581,\n",
       "  0.12380291634108465,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17712193023031123,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0704839024518581,\n",
       "  0.17712193023031123,\n",
       "  0.18522658528091326,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.12380291634108465,\n",
       "  0.17712193023031123,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.22991970177630003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.13089867598202215,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.13089867598202215,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17199611490370514,\n",
       "  0.32894072757057796,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.13089867598202215,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.13089867598202215,\n",
       "  0.22991970177630003,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17199611490370514,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.22991970177630003,\n",
       "  0.0,\n",
       "  0.13089867598202215,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.22991970177630003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.13089867598202215,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.13089867598202215,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.22991970177630003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.13089867598202215,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.22991970177630003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.32894072757057796,\n",
       "  0.32894072757057796,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.1337747560362151,\n",
       "  0.2558427881104495,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.10181008131935056,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.19188209108283716,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.10033106702716134,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.19188209108283716,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.19188209108283716,\n",
       "  0.19188209108283716,\n",
       "  0.19188209108283716,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.07635756098951292,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.07635756098951292,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.134119826036175,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.19188209108283716,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.10033106702716134,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.134119826036175,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.28782313662425574,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.20117973905426254,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.28782313662425574,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.20117973905426254,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.28782313662425574,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.20117973905426254,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.30099320108148403,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.2558427881104495,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.10181008131935056,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.10181008131935056,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.2558427881104495,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.17882643471490003,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    \n",
    "    N = len(list_of_sentence_tokens)  # The total number of documents\n",
    "    tf = []  # To hold term frequencies for each document\n",
    "    df = {}  # To hold document frequencies for each term\n",
    "    # Calculate Term Frequencies and Document Frequencies\n",
    "    for tokens in list_of_sentence_tokens:\n",
    "        token_count = {}\n",
    "        total_terms = len(tokens)\n",
    "        # print(total_terms)\n",
    "        for token in tokens:\n",
    "            if token in token_count:\n",
    "                token_count[token] += 1\n",
    "            else:\n",
    "                token_count[token] = 1\n",
    "\n",
    "            if token not in df:\n",
    "                df[token] = 0\n",
    "            df[token] += 1  # Increment document frequency for the term\n",
    "            # print(df)\n",
    "        # Calculate TF for the current document\n",
    "        tf_doc = {token: count / total_terms for token, count in token_count.items()}\n",
    "        tf.append(tf_doc)\n",
    "        # print(tf_doc)\n",
    "    # Calculate IDF for each term\n",
    "    idf = {}\n",
    "    for term, document_frequency in df.items():\n",
    "        idf[term] = math.log(N / document_frequency)  # Use custom logarithm\n",
    "\n",
    "    # Create the TF-IDF matrix\n",
    "    all_tokens = sorted(df.keys())  # Get all tokens sorted\n",
    "    tfidf_matrix = []\n",
    "\n",
    "    # Fill in the TF-IDF values in the matrix\n",
    "    for tf_doc in tf:\n",
    "        tfidf_row = []\n",
    "        for token in all_tokens:\n",
    "            tf_value = tf_doc.get(token, 0)  # Get the TF value for the token\n",
    "            tfidf_value = tf_value * idf[token]  # Calculate TF-IDF\n",
    "            tfidf_row.append(tfidf_value)  # Append to the row\n",
    "        tfidf_matrix.append(tfidf_row)\n",
    "    return tfidf_matrix\n",
    "\n",
    "get_TFIDF_matrix(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART C- Measuring Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a method that takes as an input: (15)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 2, 3, 4, 5, 6, 7, 9, 10]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based on the method given and the search query\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens \n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "    q_tokens = search_query.lower().split()\n",
    "    \n",
    "    if method_name == \"inverted\":\n",
    "        get_inverted_index(list_of_sentence_tokens)\n",
    "        \n",
    "        doc_scores = {}\n",
    "        for token in q_tokens:\n",
    "            if token in get_inverted_index(list_of_sentence_tokens):\n",
    "                for doc_id in get_inverted_index(list_of_sentence_tokens)[token]:\n",
    "                    if doc_id not in doc_scores:\n",
    "                        doc_scores[doc_id] = 0\n",
    "                    doc_scores[doc_id] += 1  # Increment the score by the count of matching tokens\n",
    "\n",
    "        # Sort documents by score in descending order and return the ranked document IDs\n",
    "        ranked_docs = sorted(doc_scores, key=doc_scores.get, reverse=True)\n",
    "        return ranked_docs\n",
    "    \n",
    "    elif method_name == \"tfidf\":\n",
    "        # Calculate the TF-IDF matrix\n",
    "        tfidf_matrix = get_TFIDF_matrix(list_of_sentence_tokens)  # Assuming this function is defined\n",
    "        all_tokens = sorted(set(token for tokens in list_of_sentence_tokens for token in tokens)) # Gather all unique tokens\n",
    "\n",
    "        # Calculate query TF-IDF\n",
    "        query_tf = {}\n",
    "        for token in q_tokens:\n",
    "            query_tf[token] = query_tf.get(token, 0) + 1\n",
    "        total_query_terms = sum(query_tf.values())\n",
    "        query_tfidf = {}\n",
    "        \n",
    "        for token, count in query_tf.items():\n",
    "            query_tfidf[token] = (count / total_query_terms)  # Normalizing TF for the query\n",
    "\n",
    "        # Calculate TF-IDF scores for documents based on the query\n",
    "        doc_scores = []\n",
    "        for doc_id, tfidf_row in enumerate(tfidf_matrix):\n",
    "            score = 0\n",
    "            for token in q_tokens:\n",
    "                if token in all_tokens:\n",
    "                    index = all_tokens.index(token)  # Get the index of the token\n",
    "                    # Ensure index access is valid\n",
    "                    if index < len(tfidf_row):  # Check index is in bounds\n",
    "                        score += tfidf_row[index] * query_tfidf.get(token, 0)  # Calculate score using TF-IDF\n",
    "            doc_scores.append((doc_id + 1, score))  # (Document ID, Score)\n",
    "            \n",
    "        # Sort documents by score in descending order and return the ranked document IDs\n",
    "        ranked_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "        return [doc_id for doc_id, _ in ranked_docs]\n",
    "\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "search_query = \"python is\"\n",
    "method_name = \"tfidf\"\n",
    "\n",
    "get_ranked_documents(tokenized, method_name, search_query)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART D- TFIDF with a TWIST (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF with Custom Weighting Based on Document Length and Term Position\n",
    "- You are expected to implement a twisted version of the TF-IDF vectorizer, that incorporates two additional features:\n",
    "    - Document Length\n",
    "    - Term Position\n",
    "- This twist aims to assign weight based on Modified Term Frequency (MTF) and Modified inverse Document Frequency (MIDF)\n",
    "1. Modified Term Frequency (MTF):\n",
    "    - MTF is calculated by taking into consideration the position of the term into account\n",
    "    - The assumption is the closer the term appears to the beginning of the document, the higher the weight should be.\n",
    "    - $$\\text{MTF}(t, d) = \\frac{f(t, d)}{1 + \\text{position}(t, d)}$$\n",
    "        - Where f(t,d) is the raw count of term t in document d.\n",
    "        - position(t,d) is the position of the first occurence of term t in document d.\n",
    "2. Modified Inverse Document Frequency (MIDF):\n",
    "    - MIDF is calculated taking into consideration the document length.\n",
    "    - The assumption is that the IDF should be inversely proportion not only to the number of documents it appears at, but also to the average length of documents where the term appears. \n",
    "    - Hence, longer documents are less significant for a term's relevance.\n",
    "    - $$\\text{MIDF}(t) = \\log \\left( \\frac{N}{\\text{df}(t) \\times \\frac{1}{M} \\sum_{d \\in D_{t}} |d|} \\right)$$\n",
    "\n",
    "        - N is the total number of documents\n",
    "        - df(t) is the document frequency\n",
    "        - M is a constant for scaling\n",
    "        - $${\\sum_{d \\in D_{t}} |d|}$$\n",
    "                 is the sum of the lengths of all documents that contain t\n",
    "        - |d| is the length of document d\n",
    "3. Final Weight (MTF-MIDF):\n",
    "    - The Combined is calculated as : MTF(t,d)*MIDF(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-A: Implement the function logic for getting modified tf-idf weightings. (20 Marks)\n",
    "<b><u>NOTE: M is a scaling factor, setting it to 5 in our example would be sufficient. However, you need to explore what does increasing and decreasing it represent.</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.04558038919848866,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.12602676010180824],\n",
       " [-0.22234291063072278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.8864200123109259, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.014529448928731495],\n",
       " [-0.04558038919848866, 0.0, 0.0, 0.0, 0.0, -0.022665737061400626, 0.0, 0.0],\n",
       " [0.0, 0.44321000615546297, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [-0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34399222980741023],\n",
       " [0.0, -0.0, -0.09038738180422692, 0.0, 0.2650329420250114],\n",
       " [0.0, 0.0, 0.0, 0.0],\n",
       " [0.0, -0.09038738180422692]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_modified_tfidf_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the modified tf-idf matrix\n",
    "    M = 10  # Define the scaling factor\n",
    "    N = len(list_of_sentence_tokens)  # Total number of documents\n",
    "    tf = []  # List to hold term frequencies for each document\n",
    "    df = defaultdict(int)  # Keep track of document frequency for each term\n",
    "    first_position = {}  # To record the first position of each term in each document\n",
    "    doc_lengths = []  # Lengths of each document in terms of number of terms\n",
    "    \n",
    "    for doc_id, tokens in enumerate(list_of_sentence_tokens):\n",
    "        length = len(tokens)\n",
    "        doc_lengths.append(length)  # Record document length\n",
    "        term_count = defaultdict(int)  # Track term counts for the current document\n",
    "        \n",
    "        for position, token in enumerate(tokens):\n",
    "            if token not in term_count:\n",
    "                term_count[token] +=1   # Count frequency\n",
    "                first_position[token] = position    # Store first occurrence position\n",
    "            else:\n",
    "                term_count[token] +=1\n",
    "        \n",
    "        for token, count in term_count.items():\n",
    "            df[token] += 1  # Update document frequency\n",
    "            tf.append((doc_id, token, count, first_position[token]))  # Append term frequency to the list\n",
    "            \n",
    "    modified_tfidf_matrix =[]\n",
    "    \n",
    "    for doc_id in range(N):\n",
    "        tfidfs = []  # Temporary list for the current document's TF-IDF values\n",
    "        unique_terms = {t for _, t, _, _ in tf if _ == doc_id}\n",
    "        \n",
    "        for token in unique_terms:\n",
    "            # Calculate MTF\n",
    "            f_t_d = sum(count for d_id, t, count, pos in tf if d_id == doc_id and t == token)\n",
    "            pos_t_d = first_position[token]  # Position of the first occurrence\n",
    "\n",
    "            # MTF = f(t, d) / (1 + position(t, d))\n",
    "            mtf = f_t_d / (1 + pos_t_d)\n",
    "            \n",
    "            # Step 4: Calculate MIDF\n",
    "            df_t = df[token]  # Document frequency of the token\n",
    "            average_length = sum(doc_lengths) / N  # Average length of documents\n",
    "            \n",
    "            # Calculate sum of lengths for documents containing the term\n",
    "            total_length_for_token = sum(doc_lengths[d_id] for d_id in [i for i in range(N) if token in list_of_sentence_tokens[i]])\n",
    "             \n",
    "            # MIDF = log(N / (df(t) * (1/M) * Sum(|d|)))\n",
    "            midf = math.log(N / (df_t * (total_length_for_token / M))) if df_t > 0 else 0\n",
    "            \n",
    "            # Step 5: Final Weight (MTF-MIDF)\n",
    "            final_weight = mtf * midf\n",
    "            tfidfs.append(final_weight)\n",
    "        \n",
    "        # Fill in the row for the current document\n",
    "        modified_tfidf_matrix.append(tfidfs)\n",
    "\n",
    "    return modified_tfidf_matrix\n",
    "        \n",
    "get_modified_tfidf_matrix(tokenized)    \n",
    "    \n",
    "    # output =[[1.254,0,0,0.564,1.11],[2.12,1.254,0.564,0,0]]\n",
    "    # return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-B: Experiment the effect of changing M and comment on what do you think M is for and why is it added. (5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A smaller `M` increases the weight of `midf`, making the significance of document length more pronounced. These weights favored more heavily when terms appear in shorter documents.\n",
    "- A larger `M` increases the weight of `midf`,  thus placing relatively more weight on the document frequency aspect.\n",
    "- Extremely large values `M` gives more emphasis on document frequency over document length. Terms that recur in longer documents may receive relatively higher scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-C: Do you think Modified TF-Modified IDF is a good technique? Please comment and explain your thoughts.(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified TF-Modified IDF (MTF-MIDF) is a useful method for assigning weight to terms in textual analysis. \n",
    "\n",
    "- MTF values the placement of terms and assigns more weight to terms found early in a document, typically indicating sensitivity to position.\n",
    "\n",
    "- Also by adjusting length of documents, MIDF minimizes the influence of terms in longer texts, focusing on more important terms that can offer better insights.\n",
    "\n",
    "- The significance of term weights by merging MTF-IDF elements, proving beneficial in tasks like information retrieval and natural language processing that require considering context.\n",
    "\n",
    "- This method can be adjusted to include extra characteristics, making it expandable for more intricate analyses, such as integrating semantic parallels.\n",
    "\n",
    "- While MTF-MIDF offers benefits, it brings about complexity and the potential for negative weights if not handled properly, requiring extensive validation and customization for individual use cases.\n",
    "\n",
    "- Lastly, MTF-MIDF improves the assessment of term importance by taking into account document organization, which is beneficial for different text-oriented assignments, as long as its restrictions are recognized and addressed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
