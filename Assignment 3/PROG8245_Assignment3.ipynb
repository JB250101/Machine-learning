{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Exploring IR & NLP\n",
    "- In this assignment we are going to implement various IR techniques <b><i>From Scratch</i></b>, Please don't use available libraries except if specified that you can use it.\n",
    "- You are required to submit 6 different functions for this assignment, you can additional helper functions but only 6 will be tested.\n",
    "- You will be granted 10 marks for clean code and documenting the code.\n",
    "- Student Name: Jaiv Burman \n",
    "- ID: 8930180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language, python proved its importance in various domains.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in python code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine\n",
      "[nltk_data]     learning\\.venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import ne_chunk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART A: Preprocessing (15 Marks)\n",
    "- You are required to preprocess the text and apply the tokenization process.<br/>\n",
    "- Proprocessing should include tokenization, normalization, stemming <b>OR</b> lemmatization, and Named Entity Recognition (NER).<br/>\n",
    "- You need to make sure that Named Entities are not broken into separate tokens, but should be normalized by case-folding only. <br/>\n",
    "- The output of this step should be list of tokenized sentences. [[sentence1_token1, sentence1_token2, .. .], [sentence2_token1, .. .], .. .] <br/>\n",
    "- Please write the functionality of clean_sentences as explained in the comment (Please do comment your code at each essential step) <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['python',\n",
       "  'is',\n",
       "  'a',\n",
       "  'versatil',\n",
       "  'program',\n",
       "  'languag',\n",
       "  ',',\n",
       "  'python',\n",
       "  'prove',\n",
       "  'it',\n",
       "  'import',\n",
       "  'in',\n",
       "  'variou',\n",
       "  'domain',\n",
       "  '.'],\n",
       " ['javascript', 'is', 'wide', 'use', 'for', 'web', 'develop', '.'],\n",
       " ['java', 'is', 'known', 'for', 'it', 'platform', 'independ', '.'],\n",
       " ['program', 'involv', 'write', 'code', 'to', 'solv', 'problem', '.'],\n",
       " ['data', 'structur', 'are', 'crucial', 'for', 'effici', 'program', '.'],\n",
       " ['algorithm',\n",
       "  'are',\n",
       "  'step-by-step',\n",
       "  'instruct',\n",
       "  'for',\n",
       "  'solv',\n",
       "  'problem',\n",
       "  '.'],\n",
       " ['version',\n",
       "  'control',\n",
       "  'system',\n",
       "  'help',\n",
       "  'manag',\n",
       "  'code',\n",
       "  'chang',\n",
       "  'in',\n",
       "  'collabor',\n",
       "  '.'],\n",
       " ['debug',\n",
       "  'is',\n",
       "  'the',\n",
       "  'process',\n",
       "  'of',\n",
       "  'find',\n",
       "  'and',\n",
       "  'fix',\n",
       "  'error',\n",
       "  'in',\n",
       "  'python',\n",
       "  'code',\n",
       "  '.'],\n",
       " ['web',\n",
       "  'framework',\n",
       "  'simplifi',\n",
       "  'the',\n",
       "  'develop',\n",
       "  'of',\n",
       "  'web',\n",
       "  'applic',\n",
       "  '.'],\n",
       " ['artifici',\n",
       "  'intellig',\n",
       "  'can',\n",
       "  'be',\n",
       "  'appli',\n",
       "  'in',\n",
       "  'variou',\n",
       "  'program',\n",
       "  'task',\n",
       "  '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## You are allowed for PART A to use any library that would help you in the task.\n",
    "def clean_sentences(sentence=None):\n",
    "    ## This function takes as an input list of sentences\n",
    "    ## This function returns a list of tokenized_sentences\n",
    "    stemmer = PorterStemmer()\n",
    "    # test_string=\" \".join(sentence)\n",
    "    tokenized_sentence=[]\n",
    "    for sentences in sentence:\n",
    "        word_tokens = word_tokenize(sentences) \n",
    "        case_folded = [word.lower() for word in word_tokens]\n",
    "        stemmed_words = [stemmer.stem(word) for word in case_folded]\n",
    "        tokenized_sentence.append(stemmed_words)\n",
    "\n",
    "    return tokenized_sentence    \n",
    "\n",
    "clean_sentences(sentence=sample_sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = clean_sentences(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART B: Building IR Sentence-Word Representation (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>inverted index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': [1, 1, 8],\n",
       " 'is': [1, 2, 3, 8],\n",
       " 'a': [1],\n",
       " 'versatil': [1],\n",
       " 'program': [1, 4, 5, 10],\n",
       " 'languag': [1],\n",
       " ',': [1],\n",
       " 'prove': [1],\n",
       " 'it': [1, 3],\n",
       " 'import': [1],\n",
       " 'in': [1, 7, 8, 10],\n",
       " 'variou': [1, 10],\n",
       " 'domain': [1],\n",
       " '.': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       " 'javascript': [2],\n",
       " 'wide': [2],\n",
       " 'use': [2],\n",
       " 'for': [2, 3, 5, 6],\n",
       " 'web': [2, 9, 9],\n",
       " 'develop': [2, 9],\n",
       " 'java': [3],\n",
       " 'known': [3],\n",
       " 'platform': [3],\n",
       " 'independ': [3],\n",
       " 'involv': [4],\n",
       " 'write': [4],\n",
       " 'code': [4, 7, 8],\n",
       " 'to': [4],\n",
       " 'solv': [4, 6],\n",
       " 'problem': [4, 6],\n",
       " 'data': [5],\n",
       " 'structur': [5],\n",
       " 'are': [5, 6],\n",
       " 'crucial': [5],\n",
       " 'effici': [5],\n",
       " 'algorithm': [6],\n",
       " 'step-by-step': [6],\n",
       " 'instruct': [6],\n",
       " 'version': [7],\n",
       " 'control': [7],\n",
       " 'system': [7],\n",
       " 'help': [7],\n",
       " 'manag': [7],\n",
       " 'chang': [7],\n",
       " 'collabor': [7],\n",
       " 'debug': [8],\n",
       " 'the': [8, 9],\n",
       " 'process': [8],\n",
       " 'of': [8, 9],\n",
       " 'find': [8],\n",
       " 'and': [8],\n",
       " 'fix': [8],\n",
       " 'error': [8],\n",
       " 'framework': [9],\n",
       " 'simplifi': [9],\n",
       " 'applic': [9],\n",
       " 'artifici': [10],\n",
       " 'intellig': [10],\n",
       " 'can': [10],\n",
       " 'be': [10],\n",
       " 'appli': [10],\n",
       " 'task': [10]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    inverted_index = {}\n",
    "    \n",
    "    for id, tokens in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "                \n",
    "            inverted_index[token].append(id)\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "get_inverted_index(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentence = get_inverted_index(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Question B-2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>Positional index</b> that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python': {1: [0, 7], 8: [10]},\n",
       " 'is': {1: [1], 2: [1], 3: [1], 8: [1]},\n",
       " 'a': {1: [2]},\n",
       " 'versatil': {1: [3]},\n",
       " 'program': {1: [4], 4: [0], 5: [6], 10: [7]},\n",
       " 'languag': {1: [5]},\n",
       " ',': {1: [6]},\n",
       " 'prove': {1: [8]},\n",
       " 'it': {1: [9], 3: [4]},\n",
       " 'import': {1: [10]},\n",
       " 'in': {1: [11], 7: [7], 8: [9], 10: [5]},\n",
       " 'variou': {1: [12], 10: [6]},\n",
       " 'domain': {1: [13]},\n",
       " '.': {1: [14],\n",
       "  2: [7],\n",
       "  3: [7],\n",
       "  4: [7],\n",
       "  5: [7],\n",
       "  6: [7],\n",
       "  7: [9],\n",
       "  8: [12],\n",
       "  9: [8],\n",
       "  10: [9]},\n",
       " 'javascript': {2: [0]},\n",
       " 'wide': {2: [2]},\n",
       " 'use': {2: [3]},\n",
       " 'for': {2: [4], 3: [3], 5: [4], 6: [4]},\n",
       " 'web': {2: [5], 9: [0, 6]},\n",
       " 'develop': {2: [6], 9: [4]},\n",
       " 'java': {3: [0]},\n",
       " 'known': {3: [2]},\n",
       " 'platform': {3: [5]},\n",
       " 'independ': {3: [6]},\n",
       " 'involv': {4: [1]},\n",
       " 'write': {4: [2]},\n",
       " 'code': {4: [3], 7: [5], 8: [11]},\n",
       " 'to': {4: [4]},\n",
       " 'solv': {4: [5], 6: [5]},\n",
       " 'problem': {4: [6], 6: [6]},\n",
       " 'data': {5: [0]},\n",
       " 'structur': {5: [1]},\n",
       " 'are': {5: [2], 6: [1]},\n",
       " 'crucial': {5: [3]},\n",
       " 'effici': {5: [5]},\n",
       " 'algorithm': {6: [0]},\n",
       " 'step-by-step': {6: [2]},\n",
       " 'instruct': {6: [3]},\n",
       " 'version': {7: [0]},\n",
       " 'control': {7: [1]},\n",
       " 'system': {7: [2]},\n",
       " 'help': {7: [3]},\n",
       " 'manag': {7: [4]},\n",
       " 'chang': {7: [6]},\n",
       " 'collabor': {7: [8]},\n",
       " 'debug': {8: [0]},\n",
       " 'the': {8: [2], 9: [3]},\n",
       " 'process': {8: [3]},\n",
       " 'of': {8: [4], 9: [5]},\n",
       " 'find': {8: [5]},\n",
       " 'and': {8: [6]},\n",
       " 'fix': {8: [7]},\n",
       " 'error': {8: [8]},\n",
       " 'framework': {9: [1]},\n",
       " 'simplifi': {9: [2]},\n",
       " 'applic': {9: [7]},\n",
       " 'artifici': {10: [0]},\n",
       " 'intellig': {10: [1]},\n",
       " 'can': {10: [2]},\n",
       " 'be': {10: [3]},\n",
       " 'appli': {10: [4]},\n",
       " 'task': {10: [8]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    positional_index = {}\n",
    "    \n",
    "    for id, tokens in enumerate(list_of_sentence_tokens, start =1):\n",
    "        for position, token in enumerate(tokens):\n",
    "            if token not in positional_index:\n",
    "                positional_index[token] = {}\n",
    "                \n",
    "            if id not in positional_index[token]:\n",
    "                positional_index[token][id] = []\n",
    "                \n",
    "            positional_index[token][id].append(position)\n",
    "            \n",
    "    return positional_index\n",
    "    \n",
    "get_positional_index(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Question B-3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the <b>TF-IDF Matrix</b> that is sufficient to represent the documents, the tokens are expected to be sorted as well as documentIDs. Assume that each sentence is a document and the sentence ID starts from 1. (10) You are not allowed to use any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the tf-idf matrix\n",
    "    output =[[1.254,0,0,0.564,1.11],[2.12,1.254,0.564,0,0]]\n",
    "    return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART C- Measuring Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a method that takes as an input: (15)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    # TODO: Implement the functionality that returns the rank of the documents based on the method given and the search query\n",
    "    ## If the method is \"inverted\" then rank the documents based on the number of matching tokens \n",
    "    ## If the method is \"tfidf\" then use the tfidf score equation in slides and return ranking based on the score\n",
    "    ## The document with highest relevance should be ranked first\n",
    "    ## list method should return the index of the documents based on highest ranking first\n",
    "    rank_list = []\n",
    "    for i in range(len(list_of_sentence_tokens)):\n",
    "        rank_list.append(i)\n",
    "    return rank_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PART D- TFIDF with a TWIST (30 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF with Custom Weighting Based on Document Length and Term Position\n",
    "- You are expected to implement a twisted version of the TF-IDF vectorizer, that incorporates two additional features:\n",
    "    - Document Length\n",
    "    - Term Position\n",
    "- This twist aims to assign weight based on Modified Term Frequency (MTF) and Modified inverse Document Frequency (MIDF)\n",
    "1. Modified Term Frequency (MTF):\n",
    "    - MTF is calculated by taking into consideration the position of the term into account\n",
    "    - The assumption is the closer the term appears to the beginning of the document, the higher the weight should be.\n",
    "    - $$\\text{MTF}(t, d) = \\frac{f(t, d)}{1 + \\text{position}(t, d)}$$\n",
    "        - Where f(t,d) is the raw count of term t in document d.\n",
    "        - position(t,d) is the position of the first occurence of term t in document d.\n",
    "2. Modified Inverse Document Frequency (MIDF):\n",
    "    - MIDF is calculated taking into consideration the document length.\n",
    "    - The assumption is that the IDF should be inversely proportion not only to the number of documents it appears at, but also to the average length of documents where the term appears. \n",
    "    - Hence, longer documents are less significant for a term's relevance.\n",
    "    - $$\\text{MIDF}(t) = \\log \\left( \\frac{N}{\\text{df}(t) \\times \\frac{1}{M} \\sum_{d \\in D_{t}} |d|} \\right)$$\n",
    "\n",
    "        - N is the total number of documents\n",
    "        - df(t) is the document frequency\n",
    "        - M is a constant for scaling\n",
    "        - $${\\sum_{d \\in D_{t}} |d|}$$\n",
    "                 is the sum of the lengths of all documents that contain t\n",
    "        - |d| is the length of document d\n",
    "3. Final Weight (MTF-MIDF):\n",
    "    - The Combined is calculated as : MTF(t,d)*MIDF(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-A: Implement the function logic for getting modified tf-idf weightings. (20 Marks)\n",
    "<b><u>NOTE: M is a scaling factor, setting it to 5 in our example would be sufficient. However, you need to explore what does increasing and decreasing it represent.</u></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_tfidf_matrix(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the modified tf-idf matrix\n",
    "    output =[[1.254,0,0,0.564,1.11],[2.12,1.254,0.564,0,0]]\n",
    "    return  output #THIS IS A PLACEHOLDER FOR THE OUTPUT YOU NEED TO OVERWRITE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-B: Experiment the effect of changing M and comment on what do you think M is for and why is it added. (5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 4-C: Do you think Modified TF-Modified IDF is a good technique? Please comment and explain your thoughts.(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b> Your answer here</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
