{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKPqr-R5FZS3"
   },
   "source": [
    "## Week 11 Lab -- Out of Ordinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFuwCXEcFgC7"
   },
   "source": [
    "- Over the past couple of day and till the end of the week, Google is running an interesting event to get users familiar with their GenAI API, I wanted to take this opportunity to introduce you to whats going on there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Sbqw-wYXE5H7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-generativeai>=0.8.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9IlqpvBoE8nS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaivb\\OneDrive\\Desktop\\Machine learning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byzIayb0FxxD"
   },
   "source": [
    "- You need to add your own GOOGLE_API_KEY here, you can get yours from [here](https://ai.google.dev/pricing#1_5flash)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qhHZbpWWFAJJ"
   },
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"AIzaSyAYy6_PtS0GwjNT9J0J0DY3at_42qscPVE\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TD45G_HF8Zv"
   },
   "source": [
    "- If you checked the site, Google provides free tier access to Gemini 1.5 flash, hence this model is the one we will be using for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5kUi_0UGYCQ"
   },
   "source": [
    "1. Prompting Single-Turn, text-in/text-out structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "zp0tvihnFKhn",
    "outputId": "6becc1b0-ecb4-4d21-f025-ad29f58490ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There isn't one single \"formula for entropy for trees\" because the concept of entropy can be applied to trees in several different ways, depending on what you want to measure.  The appropriate formula depends on the context.  Here are a few possibilities:\n",
      "\n",
      "**1. Entropy of a Decision Tree (Information Gain):**\n",
      "\n",
      "In the context of decision trees (like those used in machine learning), entropy is used to measure the impurity of a node.  The goal is to select the attribute that best splits the data, maximizing information gain (reducing entropy).\n",
      "\n",
      "* **Entropy of a single node:**  For a node with *c* classes, where *p<sub>i</sub>* is the probability of class *i* in that node:\n",
      "\n",
      "   `Entropy(node) = - Σ (p<sub>i</sub> * log<sub>2</sub>(p<sub>i</sub>))`  (summation from i=1 to c)\n",
      "\n",
      "* **Information Gain:** The difference in entropy before and after a split on an attribute.\n",
      "\n",
      "   `Information Gain(attribute) = Entropy(parent) - Σ [(number of instances in child<sub>i</sub> / number of instances in parent) * Entropy(child<sub>i</sub>)]` (summation over all child nodes)\n",
      "\n",
      "This is the most common usage of entropy related to trees in machine learning.\n",
      "\n",
      "\n",
      "**2. Entropy of a phylogenetic tree (Evolutionary Biology):**\n",
      "\n",
      "In phylogenetics, entropy can be used to quantify the uncertainty or branching complexity of an evolutionary tree.  There isn't a single universally accepted formula, but methods often involve considering the tree's topology and branch lengths.  These approaches are often more complex and might involve concepts like:\n",
      "\n",
      "* **Tree shape statistics:** Measuring the balance, asymmetry, or other properties of the tree topology.\n",
      "* **Branch length distributions:** Analyzing the distribution of evolutionary distances among the branches.\n",
      "\n",
      "These methods often rely on more advanced statistical techniques than the simple entropy formula used in decision trees.\n",
      "\n",
      "\n",
      "**3. Entropy of a data structure representing a tree:**\n",
      "\n",
      "If you're thinking about the entropy of the tree as a data structure (e.g., a binary tree, a trie), you would need to define what \"probability\" means in that context. You might consider:\n",
      "\n",
      "* **Probabilities of node values:**  If the tree nodes store data with associated probabilities, you could compute the entropy of the node values.  The formula would be similar to the decision tree entropy.\n",
      "* **Probabilities of tree shapes:**  You could consider the probability distribution of different tree shapes (e.g., perfectly balanced vs. skewed) and calculate the entropy based on this distribution.  This would be a much more involved calculation.\n",
      "\n",
      "\n",
      "In summary, you need to specify the *type* of tree and the *interpretation of probability* to determine the appropriate formula for calculating entropy.  The examples above provide different contexts and formulas.  Providing more detail about your specific use case would allow for a more precise answer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = flash.generate_content(\"formula for entropy for trees\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0C4tzF65GIOH"
   },
   "source": [
    "- GenAI returns response in MarkDown format, hence we can directly render it in our notebook/website etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "ccBLsxU2GF91",
    "outputId": "60dbdebb-4933-4538-e62c-91fe1e4dbae2"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There isn't one single \"formula for entropy for trees\" because the concept of entropy can be applied to trees in several different ways, depending on what you want to measure.  The appropriate formula depends on the context.  Here are a few possibilities:\n",
       "\n",
       "**1. Entropy of a Decision Tree (Information Gain):**\n",
       "\n",
       "In the context of decision trees (like those used in machine learning), entropy is used to measure the impurity of a node.  The goal is to select the attribute that best splits the data, maximizing information gain (reducing entropy).\n",
       "\n",
       "* **Entropy of a single node:**  For a node with *c* classes, where *p<sub>i</sub>* is the probability of class *i* in that node:\n",
       "\n",
       "   `Entropy(node) = - Σ (p<sub>i</sub> * log<sub>2</sub>(p<sub>i</sub>))`  (summation from i=1 to c)\n",
       "\n",
       "* **Information Gain:** The difference in entropy before and after a split on an attribute.\n",
       "\n",
       "   `Information Gain(attribute) = Entropy(parent) - Σ [(number of instances in child<sub>i</sub> / number of instances in parent) * Entropy(child<sub>i</sub>)]` (summation over all child nodes)\n",
       "\n",
       "This is the most common usage of entropy related to trees in machine learning.\n",
       "\n",
       "\n",
       "**2. Entropy of a phylogenetic tree (Evolutionary Biology):**\n",
       "\n",
       "In phylogenetics, entropy can be used to quantify the uncertainty or branching complexity of an evolutionary tree.  There isn't a single universally accepted formula, but methods often involve considering the tree's topology and branch lengths.  These approaches are often more complex and might involve concepts like:\n",
       "\n",
       "* **Tree shape statistics:** Measuring the balance, asymmetry, or other properties of the tree topology.\n",
       "* **Branch length distributions:** Analyzing the distribution of evolutionary distances among the branches.\n",
       "\n",
       "These methods often rely on more advanced statistical techniques than the simple entropy formula used in decision trees.\n",
       "\n",
       "\n",
       "**3. Entropy of a data structure representing a tree:**\n",
       "\n",
       "If you're thinking about the entropy of the tree as a data structure (e.g., a binary tree, a trie), you would need to define what \"probability\" means in that context. You might consider:\n",
       "\n",
       "* **Probabilities of node values:**  If the tree nodes store data with associated probabilities, you could compute the entropy of the node values.  The formula would be similar to the decision tree entropy.\n",
       "* **Probabilities of tree shapes:**  You could consider the probability distribution of different tree shapes (e.g., perfectly balanced vs. skewed) and calculate the entropy based on this distribution.  This would be a much more involved calculation.\n",
       "\n",
       "\n",
       "In summary, you need to specify the *type* of tree and the *interpretation of probability* to determine the appropriate formula for calculating entropy.  The examples above provide different contexts and formulas.  Providing more detail about your specific use case would allow for a more precise answer.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6vIP9tNGtzZ"
   },
   "source": [
    "2. Mult-turn Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqLDCrHIGijF"
   },
   "source": [
    "- ChatGPT, Gemini and all AI models work with a chat structure, we they can keep track of your history, hence a single turn prompting is not efficient lets create a chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "xWeBwY9HGh_-",
    "outputId": "9792942b-f584-4575-bdfe-65ee8158ddf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = flash.start_chat(history=[])\n",
    "response = chat.send_message(\"hi\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "W5WFi1HZGWeL"
   },
   "outputs": [],
   "source": [
    "response = chat.send_message(\"linamar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "pgHBCv-DHFxq",
    "outputId": "5d2408b6-4152-4eca-c994-5ada83e90e3b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Linamar Corporation is a Canadian-based global manufacturing company.  They're a significant player in the automotive industry, specializing in highly engineered, mission-critical parts and systems.  Their product portfolio is quite broad, encompassing things like:\n",
       "\n",
       "* **Powertrain components:**  This includes things like engine blocks, cylinder heads, transmissions, and related parts.\n",
       "* **Driveline systems:**  Components for transferring power from the engine to the wheels.\n",
       "* **Chassis components:**  Parts related to the vehicle's structure and suspension.\n",
       "* **Machining and other specialized manufacturing:** They offer contract manufacturing services beyond their own product lines.\n",
       "\n",
       "Linamar is known for its advanced manufacturing capabilities and focus on precision engineering.  They're a large, publicly traded company with operations all around the world.  Is there anything specific you'd like to know about Linamar?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "WXKw5WgQHMel",
    "outputId": "d4f0b5e9-b9b0-4101-bffd-7c364dcc388e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know your name is Jaiv.  I have no memory of past conversations.  While you've just told me,  I won't remember it for future interactions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To confirm that flash keeps track of history, lets ask the model with this chat object about my name\n",
    "response = chat.send_message(\"Do you know what my name is? (It's Jaiv)\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opQv4YQQHeF2"
   },
   "source": [
    "3. Choosing a model\n",
    "- Google Gemini API has a huge set of models from the Gemini family. You can read more about them [here](https://ai.google.dev/gemini-api/docs/models/gemini)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "vYe2eHU0Hc36",
    "outputId": "21cca2c3-9830-4b77-9dff-135304172010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemini-exp-1114\n",
      "models/gemini-exp-1121\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "KbJNMk0FHzvj",
    "outputId": "f0b99ea5-75f2-40f3-c6fe-6b8b81fdaab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "## We can also get additioal information about the model capabilities from model.list.\n",
    "for model in genai.list_models():\n",
    "  if model.name =='models/gemini-1.5-flash':\n",
    "    print(model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi7zv66OP1Yy"
   },
   "source": [
    "- Exploring Editable Generation Params:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgtbQRHrP6bX"
   },
   "source": [
    "  - Output Length:\n",
    "   When generating text with LLMs, the output length affects both cost and performance of the model. Generating more tokens increases computation, leading to higher costs.\n",
    "   \n",
    "   You can stop the model from generating excess tokens and stop in a certain limit of text using the *max_output_length* parameter. Specifying this parameter **does not** influence the generation of the tokens, hence the tokens will not be written in a more stylish or consisly written. It will only stop generating tokens when specified length is reached. Hence, Prompt Engineering may be needed to give a complete answer with the given limit.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "NtzAieufP5Aq",
    "outputId": "48c5ab85-fed5-4a58-de33-d066eccf6e9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Exploding Demand for Generative AI in the Modern World\n",
      "\n",
      "Generative AI, the ability of machines to create new content rather than simply analyze existing data, is rapidly transforming the modern world.  Its demand is not merely a fleeting trend;\n"
     ]
    }
   ],
   "source": [
    "short_model = genai.GenerativeModel('gemini-1.5-flash', generation_config=genai.GenerationConfig(max_output_tokens=50))\n",
    "response = short_model.generate_content('Write an essay on the demand of Generative AI in the modern world.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9-c8xwZgRnuL"
   },
   "outputs": [],
   "source": [
    "## Lets try to write a prompt that directs the model to answer in the max_output_tokens length (Simple introduction to Prompt Engineering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-Pe37pySYBq"
   },
   "source": [
    "  - Temperature:\n",
    "   Temperature controls the degree of randomness in token selection. Higher temperatures result in higher number of candidate token in the next word/token selection process, producing more diverse result (hence can be thought of more creativity but less prone to errors).\n",
    "\n",
    "   Temperature of 0 is a greedy decoding, selecting the most probable token at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "id": "MFQNQgBIS7UE",
    "outputId": "9f507a56-44d4-4c44-d75a-a5a705dcf36b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m## Creative Model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m high_temp_model \u001b[38;5;241m=\u001b[39m \u001b[43mgenai\u001b[49m\u001b[38;5;241m.\u001b[39mGenerativeModel(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemini-1.5-flash\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgenai\u001b[38;5;241m.\u001b[39mGenerationConfig(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m      8\u001b[0m   response \u001b[38;5;241m=\u001b[39m high_temp_model\u001b[38;5;241m.\u001b[39mgenerate_content(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPick a random colour... (answer in a single word)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'genai' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "## Creative Model\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "for _ in range(3):\n",
    "  response = high_temp_model.generate_content('Pick a random colour... (answer in a single word)')\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)\n",
    "\n",
    "  # Slow down a bit so we don't get Resource Exhausted errors.\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "I3mfZ_b4TBnj",
    "outputId": "097b7215-cd48-4fe3-8df9-82dd26ea9504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "## Greedy Model\n",
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "for _ in range(4):\n",
    "  response = low_temp_model.generate_content('Pick a random colour... (answer in a single word)')\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)\n",
    "\n",
    "  time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMlqq3f8VKhb"
   },
   "source": [
    "  - Top-K and Top-P:\n",
    "   Like temperature, Top-K and Top-P params are also used to control the diversity of the model's output.\n",
    "   \n",
    "   Top-K is a positive integer that defines the number of most probable tokens from which to select the output token from. A top-K of 1 selects a single token, performing freedy decoding.\n",
    "\n",
    "   Top-P defines the probability threshold that, once cumulative exceeded, tokens stop being selected as candidate. A Top-P of 0 is equivalent to greedy decoding, and top-P of 1 selects every token in the model's vocab.\n",
    "   \n",
    "   When both are supplied, the API will filter top-K tokens first and then top-P, then finally sample tokens using the supplied temperature.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "Lz3qhMJuVzrN",
    "outputId": "510f023a-ea19-4997-bdcd-46f60147c7c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mittens, a ginger tabby with a penchant for mischief, saw a fat, juicy bird fluttering on the fence. It was an opportunity too tempting to resist. With a swift leap, she landed on the fence, her claws finding purchase. The bird, startled, took flight. Mittens, in hot pursuit, raced across the fence, leaping onto a shed, then a trampoline, finally reaching a towering oak tree.  The bird, exhausted, landed on a branch.  Mittens, panting, had finally caught her prey.  But as she looked down at the tiny bird, she felt a pang of guilt.  A gentle nudge with her paw, and the bird soared away.  Mittens, content, sauntered home, her belly full of adventure. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Explore those params and check how the model differs.\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        # These are the default values for gemini-1.5-flash-001.\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story of 100 words about a cat who goes on an adventure.\"\n",
    "response = model.generate_content(story_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1i6hlI_WF8q"
   },
   "source": [
    "### Prompting\n",
    " We will discuss some prompting techniques that are well-known in GenAI chats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ydan221WRel"
   },
   "source": [
    "1. Zero-Shot Prompting:\n",
    "\n",
    "Zero-Shot Prompting is simply returning the request from the model directly without any further explanation.\n",
    "\n",
    "Example: Wanting the model to classify sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "I6ej_8i3WRHg",
    "outputId": "2fdf53cf-6d76-4430-a783-c59ac2139b64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **POSITIVE**\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5,\n",
    "    ))\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpDZifikWqLV"
   },
   "source": [
    "2. One-shot and Few-shot Prompting:\n",
    "\n",
    "Providing One (or few) examples to the model to understand the needed task.\n",
    "Example: Understanding Pizza Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "yZ7PK3YLWE_R",
    "outputId": "78005de9-ed35-400c-bad9-4fdcf43e9dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ))\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifhuG6YgXAgm"
   },
   "source": [
    "3. Chain of Thought Prompting (CoT):\n",
    "\n",
    "Chain-of-thought prompting tries to mimic the human reasoning around any problem, you are instructing the model to output intermediate reasoning steps, to get better results specially when combined with few-shot prompting. CoT costs more because more tokens are being generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "axJtipz8XABP",
    "outputId": "bb8c9d91-4164-45f4-8ca2-9644381019ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer only.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "RG-Hwo33XeG-",
    "outputId": "771b8f1d-3cfe-4532-820d-c5212493869f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve this:\n",
      "\n",
      "* **When you were 4, your partner was 3 times your age:** 4 years old * 3 = 12 years old.\n",
      "* **The age difference between you and your partner:** 12 years old - 4 years old = 8 years.\n",
      "* **Since the age difference remains the same, your partner is still 8 years older than you.**\n",
      "* **Your partner's current age:** 20 years old + 8 years = 28 years old.\n",
      "\n",
      "**Therefore, your partner is 28 years old.** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcnFbaFJXrw_"
   },
   "source": [
    "Useful Resources:\n",
    "* [Introduction to Prompting](https://ai.google.dev/gemini-api/docs/prompting-intro) from the Gemini API docs,\n",
    "* [Gemeni's API Prompt Gallery](https://ai.google.dev/gemini-api/prompts)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
